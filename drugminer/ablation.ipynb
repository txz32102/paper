{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([320])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[6], return_contacts=True)\n",
    "token_representations = results[\"representations\"][6]\n",
    "\n",
    "sequence_representations = []\n",
    "for i, tokens_len in enumerate(batch_lens):\n",
    "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "print(len(sequence_representations))\n",
    "print(sequence_representations[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 73, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import esm\n",
    "from esm.modules import ContactPredictionHead, ESM1bLayerNorm, RobertaLMHead, TransformerLayer\n",
    "\n",
    "class ESM2_test(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        embed_dim: int = 320,  # Change the embed_dim to match the ESM2 module\n",
    "        attention_heads: int = 4,  # Change the attention_heads to match the ESM2 module\n",
    "        alphabet: Union[esm.data.Alphabet, str] = \"ESM-1b\",\n",
    "        token_dropout: bool = True,\n",
    "        linear_in1: int = 320, \n",
    "        linear_in2: int = 180, \n",
    "        linear_in3: int = 60, \n",
    "        linear_in4: int = 30, \n",
    "        linear_out: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention_heads = attention_heads\n",
    "        if not isinstance(alphabet, esm.data.Alphabet):\n",
    "            alphabet = esm.data.Alphabet.from_architecture(alphabet)\n",
    "        self.alphabet = alphabet\n",
    "        self.alphabet_size = len(alphabet)\n",
    "        self.padding_idx = alphabet.padding_idx\n",
    "        self.mask_idx = alphabet.mask_idx\n",
    "        self.cls_idx = alphabet.cls_idx\n",
    "        self.eos_idx = alphabet.eos_idx\n",
    "        self.prepend_bos = alphabet.prepend_bos\n",
    "        self.append_eos = alphabet.append_eos\n",
    "        self.token_dropout = token_dropout\n",
    "        self.layer1 = nn.Linear(linear_in1, linear_in2)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(linear_in2, linear_in3)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(linear_in3, linear_in4)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(linear_in4, linear_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self._init_submodules()\n",
    "\n",
    "    def _init_submodules(self):\n",
    "        self.embed_scale = 1\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            self.alphabet_size,\n",
    "            self.embed_dim,\n",
    "            padding_idx=self.padding_idx,\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(\n",
    "                    self.embed_dim,\n",
    "                    4 * self.embed_dim,\n",
    "                    self.attention_heads,\n",
    "                    add_bias_kv=False,\n",
    "                    use_esm1b_layer_norm=True,\n",
    "                    use_rotary_embeddings=True,\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.contact_head = ContactPredictionHead(\n",
    "            self.num_layers * self.attention_heads,\n",
    "            self.prepend_bos,\n",
    "            self.append_eos,\n",
    "            eos_idx=self.eos_idx,\n",
    "        )\n",
    "        self.emb_layer_norm_after = ESM1bLayerNorm(self.embed_dim)\n",
    "\n",
    "        self.lm_head = RobertaLMHead(\n",
    "            embed_dim=self.embed_dim,\n",
    "            output_dim=self.alphabet_size,\n",
    "            weight=self.embed_tokens.weight,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, repr_layers=[], need_head_weights=False, return_contacts=False):\n",
    "        if return_contacts:\n",
    "            need_head_weights = True\n",
    "\n",
    "        assert tokens.ndim == 2\n",
    "        padding_mask = tokens.eq(self.padding_idx)  # B, T\n",
    "\n",
    "        x = self.embed_scale * self.embed_tokens(tokens)\n",
    "\n",
    "        if self.token_dropout:\n",
    "            x.masked_fill_((tokens == self.mask_idx).unsqueeze(-1), 0.0)\n",
    "            mask_ratio_train = 0.15 * 0.8\n",
    "            src_lengths = (~padding_mask).sum(-1)\n",
    "            mask_ratio_observed = (tokens == self.mask_idx).sum(-1).to(x.dtype) / src_lengths\n",
    "            x = x * (1 - mask_ratio_train) / (1 - mask_ratio_observed)[:, None, None]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n",
    "\n",
    "        repr_layers = set(repr_layers)\n",
    "        hidden_representations = {}\n",
    "        if 0 in repr_layers:\n",
    "            hidden_representations[0] = x\n",
    "\n",
    "        if need_head_weights:\n",
    "            attn_weights = []\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        if not padding_mask.any():\n",
    "            padding_mask = None\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            x, attn = layer(\n",
    "                x,\n",
    "                self_attn_padding_mask=padding_mask,\n",
    "                need_head_weights=need_head_weights,\n",
    "            )\n",
    "            if (layer_idx + 1) in repr_layers:\n",
    "                hidden_representations[layer_idx + 1] = x.transpose(0, 1)\n",
    "            if need_head_weights:\n",
    "                attn_weights.append(attn.transpose(1, 0))\n",
    "\n",
    "        x = self.emb_layer_norm_after(x)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        if (layer_idx + 1) in repr_layers:\n",
    "            hidden_representations[layer_idx + 1] = x\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        result = {\"logits\": x, \"representations\": hidden_representations}\n",
    "        if need_head_weights:\n",
    "            attentions = torch.stack(attn_weights, 1)\n",
    "            if padding_mask is not None:\n",
    "                attention_mask = 1 - padding_mask.type_as(attentions)\n",
    "                attention_mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2)\n",
    "                attentions = attentions * attention_mask[:, None, None, :, :]\n",
    "            result[\"attentions\"] = attentions\n",
    "            if return_contacts:\n",
    "                contacts = self.contact_head(tokens, attentions)\n",
    "                result[\"contacts\"] = contacts\n",
    "        x = result[\"representations\"][6]\n",
    "        sequence_representations = []\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "    def predict_contacts(self, tokens):\n",
    "        return self(tokens, return_contacts=True)[\"contacts\"]\n",
    "\n",
    "\n",
    "model_test = ESM2_test()\n",
    "with torch.no_grad():\n",
    "    results = model_test(batch_tokens, repr_layers=[6], return_contacts=True)\n",
    "\n",
    "results.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
