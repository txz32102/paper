{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, \\\n",
    "                    matthews_corrcoef, ConfusionMatrixDisplay,f1_score, \\\n",
    "                    accuracy_score, recall_score, precision_score, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import Module, Conv1d, Linear, Dropout, MaxPool1d, functional as F, BatchNorm1d, LazyLinear\n",
    "from torch.optim import Optimizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = os.path.join(os.getcwd(), 'CNN.pt')\n",
    "train_loss_log = os.path.join(os.getcwd(), \"train_log.txt\")\n",
    "test_loss_log = os.path.join(os.getcwd(), \"validation_log.txt\")\n",
    "\n",
    "plot_range = 10000  # range(1, 10000)\n",
    "stratify = True\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "epochs = 500\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data = torch.from_numpy(x).float()\n",
    "        self.labels = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "def get_th_dataset(x, y):\n",
    "    \"\"\"\n",
    "    assemble a dataset with the given data and labels\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _dataset = CustomDataset(x, y)\n",
    "    return _dataset\n",
    "\n",
    "\n",
    "def get_dataloader(dataset: Dataset, batch_size):\n",
    "    \"\"\"\n",
    "    assemble a dataloader with the given dataset\n",
    "    :param dataset:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _dataLoader = DataLoader(dataset=dataset, batch_size=batch_size, pin_memory=True,\n",
    "                             drop_last=True, shuffle=True)\n",
    "    return _dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ASLSingleLabel(nn.Module):\n",
    "    '''\n",
    "    This loss is intended for single-label classification problems\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=0, gamma_neg=4, eps: float = 0.1, reduction='mean'):\n",
    "        super(ASLSingleLabel, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.targets_classes = []\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        '''\n",
    "        \"input\" dimensions: - (batch_size,number_classes)\n",
    "        \"target\" dimensions: - (batch_size)\n",
    "        '''\n",
    "        num_classes = inputs.size()[-1]\n",
    "        log_preds = self.logsoftmax(inputs)\n",
    "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1)\n",
    "\n",
    "        # ASL weights\n",
    "        targets = self.targets_classes\n",
    "        anti_targets = 1 - targets\n",
    "        xs_pos = torch.exp(log_preds)\n",
    "        xs_neg = 1 - xs_pos\n",
    "        xs_pos = xs_pos * targets\n",
    "        xs_neg = xs_neg * anti_targets\n",
    "        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n",
    "                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
    "        log_preds = log_preds * asymmetric_w\n",
    "\n",
    "        if self.eps > 0:  # label smoothing\n",
    "            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = - self.targets_classes.mul(log_preds)\n",
    "\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Cnn(Module):\n",
    "    \"\"\"\n",
    "    CNN model\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim=1, input_dim=320, drop_out=0, stride=2):\n",
    "        super(Cnn, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.drop_out = drop_out\n",
    "\n",
    "        self.kernel_1 = 3\n",
    "        self.channel_1 = 32\n",
    "\n",
    "        self.conv_1 = Conv1d(kernel_size=self.kernel_1, out_channels=self.channel_1, in_channels=1, stride=1)\n",
    "        self.normalizer_1 = BatchNorm1d(self.channel_1)\n",
    "        self.pooling_1 = MaxPool1d(kernel_size=self.kernel_1, stride=stride)\n",
    "\n",
    "        self.dropout = Dropout(p=drop_out)\n",
    "        self.fc1 = LazyLinear(64)\n",
    "        self.normalizer_2 = BatchNorm1d(64)\n",
    "        self.fc2 = Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, dim=1)  # (batch, embedding_dim) -> (batch, 1, embedding_dim)\n",
    "        c_1 = self.pooling_1(F.relu(self.normalizer_1(self.conv_1(x))))\n",
    "\n",
    "        c_2 = torch.flatten(c_1, start_dim=1)\n",
    "        c_2 = self.dropout(c_2)\n",
    "        out = F.relu(self.normalizer_2(self.fc1(c_2)))\n",
    "        out = self.fc2(out)\n",
    "        out = torch.softmax(out, dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def to_log(epoch: int, loss: float, accuracy, logFile: str, is_append: bool):\n",
    "    info = str(epoch) + ' ' + str(loss) + ' ' + str(accuracy) + '\\n'\n",
    "    flag = 'a' if is_append else 'w'\n",
    "    file = open(logFile, flag)  # append mode\n",
    "    file.write(info)\n",
    "    file.close()\n",
    "\n",
    "def scores(y_pred: th.Tensor, y_test: th.Tensor):\n",
    "    predictions = th.argmax(y_pred, dim=-1).numpy()\n",
    "    labels = y_test.numpy()\n",
    "    # labels = th.argmax(y_test, dim=-1).numpy()\n",
    "    recall = recall_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    precision = precision_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    f1 = f1_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    accuracy = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    # auc_score = roc_auc_score(y_score=y_pred.detach().numpy(), y_true=y_test.detach().numpy())\n",
    "    corr = matthews_corrcoef(y_true=labels, y_pred=predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true=labels, y_pred=predictions, )\n",
    "\n",
    "    report = {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        # \"auc\": auc_score,\n",
    "        'matthews_corrcoef': corr,\n",
    "        'balanced_accuracy': balanced_accuracy\n",
    "    }\n",
    "    return report\n",
    "\n",
    "\n",
    "def report(model: torch.nn.Module, dataset: CustomDataset):\n",
    "    _inputs, _labels = dataset.get_data(), dataset.get_labels()\n",
    "    print(_inputs.size(0))\n",
    "    predictions = model(_inputs)\n",
    "    res = scores(predictions, _labels.squeeze())\n",
    "    print('accuracy ' + str(res[\"accuracy\"]))\n",
    "    print('precision ' + str(res[\"precision\"]))\n",
    "    print('f1 ' + str(res[\"f1\"]))\n",
    "    print('recall ' + str(res[\"recall\"]))\n",
    "    # print('auc_score ' + str(res[\"auc\"]))\n",
    "    print('matthews_corrcoef ' + str(res[\"matthews_corrcoef\"]))\n",
    "    print('balanced_accuracy ' + str(res[\"balanced_accuracy\"]))\n",
    "    # get_confusion_matrix(predictions, _labels.squeeze())\n",
    "\n",
    "\n",
    "def train(model: Module, EPOCHS, optimizer: Optimizer, criteria,\n",
    "           checkpoint, train_set: DataLoader, vali_set: DataLoader, device, LOG_VALIDATION, LOG_TRAIN):\n",
    "    \"\"\"\n",
    "    fine tune the model and save the best model in the checkpoint\n",
    "    :param LOG_TRAIN:\n",
    "    :param LOG_VALIDATION:\n",
    "    :param device:\n",
    "    :param model: a Cnn or ConvLSTM model\n",
    "    :param EPOCHS: hyperparameter Epoch\n",
    "    :param optimizer: pytorch optimizer\n",
    "    :param criteria: loss function\n",
    "    :param checkpoint: model checkpoint\n",
    "    :param train_set: a dataloader\n",
    "    :param vali_set: a dataloader\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if os.path.exists(LOG_VALIDATION):\n",
    "        os.remove(LOG_VALIDATION)\n",
    "    if os.path.exists(LOG_TRAIN):\n",
    "        os.remove(LOG_TRAIN)\n",
    "    model = model.to(device)\n",
    "    min_vali_loss = float(\"inf\")\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        running_loss = 0.0\n",
    "        train_acc = []\n",
    "        vali_loss = 0.0\n",
    "        model.train()\n",
    "        counter = 0\n",
    "        for i, (inputs, labels) in enumerate(train_set):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # outputs = outputs.squeeze()\n",
    "            loss = criteria(outputs.float(), labels.float().squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_acc.append(scores(outputs.to(\"cpu\"), labels.to(\"cpu\"))[\"accuracy\"])\n",
    "            counter = i\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        for j, (vali_inputs, vali_labels) in enumerate(vali_set):\n",
    "            vali_labels = vali_labels.to(device)\n",
    "            vali_inputs = vali_inputs.to(device)\n",
    "            vali_outputs = model(vali_inputs)\n",
    "            # vali_outputs = vali_outputs.squeeze()\n",
    "            acc = scores(vali_outputs.to('cpu'), vali_labels.to('cpu'))[\"accuracy\"]\n",
    "            vali_loss = criteria(vali_outputs.to(device).float(), vali_labels.to(device).float().squeeze())\n",
    "            if vali_loss < min_vali_loss:\n",
    "                min_vali_loss = vali_loss\n",
    "                th.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "        avg_loss = running_loss / counter  # loss per batch\n",
    "        train_accuracy = sum(train_acc) / len(train_acc)\n",
    "        # print('epoch {} train_loss: {} vali_loss: {} test_acc: {}'\n",
    "        #       .format(epoch + 1, f'{avg_loss:5f}', f'{vali_loss:5f}', f'{acc: 5f}'))\n",
    "        # logs\n",
    "        to_log(epoch, avg_loss, train_accuracy, LOG_TRAIN, True)\n",
    "        to_log(epoch, vali_loss.item(), acc, LOG_VALIDATION, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/drugfinder/esm2_320_dimensions_with_labels.csv') \n",
    "X = df.drop(['label', 'UniProt_id'], axis=1)\n",
    "y = df['label'].apply(lambda x: 0 if x != 1 else x).to_numpy().astype(np.int64)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scalar = MinMaxScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "train_set = get_th_dataset(X_train, y_train)\n",
    "test_set = get_th_dataset(X_test, y_test)\n",
    "train_loader = get_dataloader(train_set, batch_size=batch_size)\n",
    "test_loader = get_dataloader(test_set, batch_size=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 12:51:46.317808: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-20 12:51:46.339308: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-20 12:51:46.339330: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-20 12:51:46.339346: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-20 12:51:46.344081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Cnn(output_dim=1, input_dim=320, drop_out=0, stride=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criteria = ASLSingleLabel(gamma_pos=1, gamma_neg=1, eps = 0.1)  # find the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [06:12<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(model=model, EPOCHS=epochs, optimizer=optimizer,  criteria=criteria,\n",
    "      train_set=train_loader,  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cnn(\n",
       "  (conv_1): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n",
       "  (normalizer_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pooling_1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (fc1): LazyLinear(in_features=0, out_features=64, bias=True)\n",
       "  (normalizer_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpoint)\n",
    "saved_model = Cnn(output_dim=1, input_dim=320, drop_out=0, stride=2)\n",
    "saved_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = th.load(checkpoint)\n",
    "saved_model = Cnn(output_dim=1, input_dim=320, drop_out=0, stride=2)\n",
    "saved_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_th_dataset(X_train, y_train)\n",
    "test_set = get_th_dataset(X_test, y_test)\n",
    "report(saved_model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On test set\n",
    "\"\"\"\n",
    "report(model, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
